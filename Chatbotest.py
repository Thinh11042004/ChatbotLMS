import os
import re
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.memory import ConversationBufferMemory
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.retrieval import create_retrieval_chain

from reranker import SentenceTransformersReranker
from prompts import (
    QA_CHAIN_PROMPT,
    SUMMARY_PROMPT,
    STUDENT_SUPPORT_PROMPT,
    PREREQUISITE_PROMPT,
    CLO_EXPLAIN_PROMPT,
    ROADMAP_PROMPT,
    WARNING_PROMPT,
    COMPARISON_PROMPT,
    SKILL_PROMPT,
    WORKLOAD_PROMPT,
    TOPIC_LIST_PROMPT
)

# === Load bi·∫øn m√¥i tr∆∞·ªùng ===
load_dotenv()

# === H√†m ch·ªçn Prompt ===
def choose_prompt(query: str):
    q = query.lower()
    if re.search(r"(t√≥m t·∫Øt|n·ªôi dung ch√≠nh|m√¥n n√†y n√≥i v·ªÅ g√¨|gi·ªõi thi·ªáu)", q):
        return SUMMARY_PROMPT
    elif re.search(r"(ti√™n quy·∫øt|m√¥n.*tr∆∞·ªõc|ph·∫£i h·ªçc.*tr∆∞·ªõc|c·∫ßn h·ªçc.*tr∆∞·ªõc)", q):
        return PREREQUISITE_PROMPT
    elif re.search(r"(chu·∫©n ƒë·∫ßu ra|clo|ƒë·∫°t ƒë∆∞·ª£c g√¨|sau m√¥n n√†y)", q):
        return CLO_EXPLAIN_PROMPT
    elif re.search(r"(t∆∞ v·∫•n|l·ªô tr√¨nh|h·ªçc ti·∫øp|n√™n h·ªçc g√¨|x·∫øp m√¥n|th·ª© t·ª± h·ªçc)", q):
        return ROADMAP_PROMPT
    elif re.search(r"(kh√≥|c·∫£nh b√°o|√°p l·ª±c|m√¥n n·∫∑ng|c√≥ kh√≥ kh√¥ng)", q):
        return WARNING_PROMPT
    elif re.search(r"(so s√°nh|kh√°c nhau|kh√°c bi·ªát|m√¥n n√†o t·ªët h∆°n|m√¥n n√†o d·ªÖ h∆°n)", q):
        return COMPARISON_PROMPT
    elif re.search(r"(k·ªπ nƒÉng|ƒë·∫°t ƒë∆∞·ª£c g√¨|·ª©ng d·ª•ng|ra tr∆∞·ªùng l√†m g√¨|m√¥n n√†y d√πng ƒë·ªÉ l√†m g√¨)", q):
        return SKILL_PROMPT
    elif re.search(r"(kh·ªëi l∆∞·ª£ng|t·ªën th·ªùi gian|n·∫∑ng|c√≥ nhi·ªÅu b√†i t·∫≠p|ph·∫£i h·ªçc nhi·ªÅu)", q):
        return WORKLOAD_PROMPT
    elif re.search(r"(nƒÉm nh·∫•t|g·ª£i √Ω h·ªçc|n√™n h·ªçc tr∆∞·ªõc|m·ªõi v√†o n√™n h·ªçc)", q):
        return STUDENT_SUPPORT_PROMPT
    elif re.search(r"(topic|ch·ªß ƒë·ªÅ|n·ªôi dung|h·ªçc g√¨|bao g·ªìm|b√†i h·ªçc|m√¥n h·ªçc n√†y c√≥ g√¨)", q):
        return TOPIC_LIST_PROMPT
    else:
        return QA_CHAIN_PROMPT

# === LLM ===
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro-latest",
    temperature=0,
    google_api_key=os.getenv("GOOGLE_API_KEY")
)

# === Embedding & FAISS ===
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)

# === Reranker & Retriever ===
reranker = SentenceTransformersReranker(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")
retriever = db.as_retriever(search_kwargs={"k": 6})
compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=retriever,
)

# === Memory ===
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# === CLI ===
print("üîç ƒê·∫∑t c√¢u h·ªèi v·ªÅ h·ªçc ph·∫ßn (g√µ 'exit' ƒë·ªÉ tho√°t):\n")

while True:
    query = input("üßë‚Äçüè´ B·∫°n h·ªèi g√¨? > ").strip()
    if query.lower() == "exit":
        break

    # --- Ch·ªçn Prompt ph√π h·ª£p ---
    prompt = choose_prompt(query)

    # === Hi·ªÉn th·ªã t√†i li·ªáu g·ªëc t·ª´ FAISS ===
    print("\n[1] üîç RETRIEVAL (T·ª´ FAISS):")
    retrieved_docs = retriever.invoke(query)
    for i, doc in enumerate(retrieved_docs):
        print(f"--- Doc {i+1} ---\n{doc.page_content[:300]} ...\n")

    # === Sau khi rerank ===
    print("[2] üìä RERANKED (Sau khi rerank):")
    reranked_docs = compression_retriever.invoke(query)
    for i, doc in enumerate(reranked_docs):
        print(f"‚≠ê Top {i+1}\n{doc.page_content[:300]} ...\n")

    # === T·∫°o history-aware retriever ===
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", "B·∫°n l√† m·ªôt tr·ª£ l√Ω h·ªçc t·∫≠p, tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n b·ªëi c·∫£nh h·ªôi tho·∫°i."),
        ("human", "{question}")
    ])


    history_aware_retriever = create_history_aware_retriever(
        llm=llm,
        retriever=compression_retriever,
        prompt=qa_prompt
    )

    # === Chain t·ªïng h·ª£p t√†i li·ªáu b·∫±ng prompt ƒë·ªông
    combine_docs_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt
    )

    # === T·∫°o chain ch√≠nh
    qa_chain = create_retrieval_chain(
        retriever=history_aware_retriever,
        combine_docs_chain=combine_docs_chain
    )

    # === G·ªçi chain
    print("[3] ü§ñ LLM ANSWER:")
    result = qa_chain.invoke({
        "question": query,
        "chat_history": memory.chat_memory.messages
    })
    print(result["answer"])

    # === Hi·ªÉn th·ªã ngu·ªìn
    print("\n[4] üìå Ngu·ªìn t√†i li·ªáu ")
    for doc in reranked_docs[:3]:
        print("- " + doc.page_content[:150].replace("\n", " ") + "...")

    # === Logging ra file
    with open("log.txt", "a", encoding="utf-8") as f:
        f.write(f"\n=== LOG {query} ===\n")
        f.write(f"Prompt: {prompt.template[:50]}...\n")
        f.write(f"RETRIEVED: {len(retrieved_docs)} docs\n")
        f.write(f"RERANKED TOP: {reranked_docs[0].page_content[:120]}...\n")
        f.write(f"ANSWER: {result['answer']}\n")
        f.write("="*50 + "\n")

    print("\n" + "="*80 + "\n")
